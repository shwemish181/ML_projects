{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR',\n",
       " 'data',\n",
       " 'data_filename',\n",
       " 'feature_names',\n",
       " 'target',\n",
       " 'target_filename']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "dir(diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_input=pd.DataFrame(diabetes.data,columns=diabetes.feature_names)\n",
    "df_target=pd.DataFrame(diabetes.target,columns=[\"DiseaseProgression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.092695</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.040696</td>\n",
       "      <td>-0.019442</td>\n",
       "      <td>-0.068991</td>\n",
       "      <td>-0.079288</td>\n",
       "      <td>0.041277</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.041180</td>\n",
       "      <td>-0.096346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.047163</td>\n",
       "      <td>-0.015999</td>\n",
       "      <td>-0.040096</td>\n",
       "      <td>-0.024800</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.062913</td>\n",
       "      <td>-0.038357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.063504</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.066630</td>\n",
       "      <td>0.090620</td>\n",
       "      <td>0.108914</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>-0.035817</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>-0.040099</td>\n",
       "      <td>-0.013953</td>\n",
       "      <td>0.006202</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.014956</td>\n",
       "      <td>0.011349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.070900</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>-0.033214</td>\n",
       "      <td>-0.012577</td>\n",
       "      <td>-0.034508</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.067736</td>\n",
       "      <td>-0.013504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.096328</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.083808</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>-0.103389</td>\n",
       "      <td>-0.090561</td>\n",
       "      <td>-0.013948</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.062913</td>\n",
       "      <td>-0.034215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.027178</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.017506</td>\n",
       "      <td>-0.033214</td>\n",
       "      <td>-0.007073</td>\n",
       "      <td>0.045972</td>\n",
       "      <td>-0.065491</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>-0.096433</td>\n",
       "      <td>-0.059067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.016281</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.028840</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>-0.009769</td>\n",
       "      <td>0.044958</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.030751</td>\n",
       "      <td>-0.042499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>-0.015719</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.038393</td>\n",
       "      <td>-0.013504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.045341</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.025607</td>\n",
       "      <td>-0.012556</td>\n",
       "      <td>0.017694</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.081775</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.075636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.052738</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.018062</td>\n",
       "      <td>0.080401</td>\n",
       "      <td>0.089244</td>\n",
       "      <td>0.107662</td>\n",
       "      <td>-0.039719</td>\n",
       "      <td>0.108111</td>\n",
       "      <td>0.036056</td>\n",
       "      <td>-0.042499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.042296</td>\n",
       "      <td>0.049415</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>-0.023861</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>0.052280</td>\n",
       "      <td>0.027917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.070769</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.056301</td>\n",
       "      <td>0.034206</td>\n",
       "      <td>0.049416</td>\n",
       "      <td>-0.039719</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.027368</td>\n",
       "      <td>-0.001078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.038207</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.010517</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.019476</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.027310</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.018062</td>\n",
       "      <td>-0.040099</td>\n",
       "      <td>-0.002945</td>\n",
       "      <td>-0.011335</td>\n",
       "      <td>0.037595</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.008944</td>\n",
       "      <td>-0.054925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.049105</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.056863</td>\n",
       "      <td>-0.043542</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.043276</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.011901</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.085430</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.022373</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.026366</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.072128</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.085430</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.004050</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>-0.002945</td>\n",
       "      <td>0.007767</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.061177</td>\n",
       "      <td>-0.013504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.045341</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.060618</td>\n",
       "      <td>0.031053</td>\n",
       "      <td>0.028702</td>\n",
       "      <td>-0.047347</td>\n",
       "      <td>-0.054446</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.133599</td>\n",
       "      <td>0.135612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.063635</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.035829</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>-0.030464</td>\n",
       "      <td>-0.018850</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.025952</td>\n",
       "      <td>-0.054925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.067268</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.012673</td>\n",
       "      <td>-0.040099</td>\n",
       "      <td>-0.015328</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>-0.058127</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.019199</td>\n",
       "      <td>-0.034215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.107226</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.077342</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.089630</td>\n",
       "      <td>-0.096198</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.042572</td>\n",
       "      <td>-0.005220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.023677</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.059541</td>\n",
       "      <td>-0.040099</td>\n",
       "      <td>-0.042848</td>\n",
       "      <td>-0.043589</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.015998</td>\n",
       "      <td>0.040343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.052606</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.021295</td>\n",
       "      <td>-0.074528</td>\n",
       "      <td>-0.040096</td>\n",
       "      <td>-0.037639</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>-0.054925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.067136</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.006206</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>-0.042848</td>\n",
       "      <td>-0.095885</td>\n",
       "      <td>0.052322</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>0.059424</td>\n",
       "      <td>0.052770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.074401</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.085408</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.014942</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.006209</td>\n",
       "      <td>0.085907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>-0.052738</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.000817</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>0.010815</td>\n",
       "      <td>0.007141</td>\n",
       "      <td>0.048640</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.035817</td>\n",
       "      <td>0.019633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.081666</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>-0.004523</td>\n",
       "      <td>0.109883</td>\n",
       "      <td>0.117056</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>0.091875</td>\n",
       "      <td>0.054724</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.008883</td>\n",
       "      <td>-0.050428</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>0.047224</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>-0.027310</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.080019</td>\n",
       "      <td>0.098763</td>\n",
       "      <td>-0.002945</td>\n",
       "      <td>0.018101</td>\n",
       "      <td>-0.017629</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>-0.029528</td>\n",
       "      <td>0.036201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>-0.052738</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.071397</td>\n",
       "      <td>-0.074528</td>\n",
       "      <td>-0.015328</td>\n",
       "      <td>-0.001314</td>\n",
       "      <td>0.004460</td>\n",
       "      <td>-0.021412</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.009016</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.024529</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>0.098876</td>\n",
       "      <td>0.094196</td>\n",
       "      <td>0.070730</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.021394</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>-0.020045</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.054707</td>\n",
       "      <td>-0.053871</td>\n",
       "      <td>-0.066239</td>\n",
       "      <td>-0.057367</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.074089</td>\n",
       "      <td>-0.005220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.023546</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.034698</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.033249</td>\n",
       "      <td>0.061054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.039710</td>\n",
       "      <td>0.045032</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>0.071210</td>\n",
       "      <td>0.049769</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>-0.078165</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.077863</td>\n",
       "      <td>0.052858</td>\n",
       "      <td>0.078236</td>\n",
       "      <td>0.064447</td>\n",
       "      <td>0.026550</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.040672</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.009016</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.039618</td>\n",
       "      <td>0.028758</td>\n",
       "      <td>0.038334</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>-0.072854</td>\n",
       "      <td>0.108111</td>\n",
       "      <td>0.015567</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.011039</td>\n",
       "      <td>-0.019442</td>\n",
       "      <td>-0.016704</td>\n",
       "      <td>-0.003819</td>\n",
       "      <td>-0.047082</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.023775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>-0.078165</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.040696</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>-0.100638</td>\n",
       "      <td>-0.112795</td>\n",
       "      <td>0.022869</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.020289</td>\n",
       "      <td>-0.050783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.030811</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.034229</td>\n",
       "      <td>0.043677</td>\n",
       "      <td>0.057597</td>\n",
       "      <td>0.068831</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>0.057557</td>\n",
       "      <td>0.035462</td>\n",
       "      <td>0.085907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>-0.034575</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.073119</td>\n",
       "      <td>-0.062691</td>\n",
       "      <td>-0.006584</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.045421</td>\n",
       "      <td>0.032059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.048974</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.088642</td>\n",
       "      <td>0.087287</td>\n",
       "      <td>0.035582</td>\n",
       "      <td>0.021546</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.066048</td>\n",
       "      <td>0.131470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>-0.041840</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.033151</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.041587</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>-0.024733</td>\n",
       "      <td>-0.025952</td>\n",
       "      <td>-0.038357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>-0.009147</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.056863</td>\n",
       "      <td>-0.050428</td>\n",
       "      <td>0.021822</td>\n",
       "      <td>0.045345</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.009919</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.070769</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.030996</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.047034</td>\n",
       "      <td>0.033914</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.014956</td>\n",
       "      <td>-0.001078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.009016</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.055229</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>0.057597</td>\n",
       "      <td>0.044719</td>\n",
       "      <td>-0.002903</td>\n",
       "      <td>0.023239</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>0.106617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>-0.027310</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.060097</td>\n",
       "      <td>-0.029771</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.019980</td>\n",
       "      <td>0.122273</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.051401</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>0.016281</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.010899</td>\n",
       "      <td>0.030232</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.045421</td>\n",
       "      <td>0.032059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>-0.012780</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.023451</td>\n",
       "      <td>-0.040099</td>\n",
       "      <td>-0.016704</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>-0.017629</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.038459</td>\n",
       "      <td>-0.038357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>-0.056370</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.074108</td>\n",
       "      <td>-0.050428</td>\n",
       "      <td>-0.024960</td>\n",
       "      <td>-0.047034</td>\n",
       "      <td>0.092820</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>-0.061177</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "5   -0.092695 -0.044642 -0.040696 -0.019442 -0.068991 -0.079288  0.041277   \n",
       "6   -0.045472  0.050680 -0.047163 -0.015999 -0.040096 -0.024800  0.000779   \n",
       "7    0.063504  0.050680 -0.001895  0.066630  0.090620  0.108914  0.022869   \n",
       "8    0.041708  0.050680  0.061696 -0.040099 -0.013953  0.006202 -0.028674   \n",
       "9   -0.070900 -0.044642  0.039062 -0.033214 -0.012577 -0.034508 -0.024993   \n",
       "10  -0.096328 -0.044642 -0.083808  0.008101 -0.103389 -0.090561 -0.013948   \n",
       "11   0.027178  0.050680  0.017506 -0.033214 -0.007073  0.045972 -0.065491   \n",
       "12   0.016281 -0.044642 -0.028840 -0.009113 -0.004321 -0.009769  0.044958   \n",
       "13   0.005383  0.050680 -0.001895  0.008101 -0.004321 -0.015719 -0.002903   \n",
       "14   0.045341 -0.044642 -0.025607 -0.012556  0.017694 -0.000061  0.081775   \n",
       "15  -0.052738  0.050680 -0.018062  0.080401  0.089244  0.107662 -0.039719   \n",
       "16  -0.005515 -0.044642  0.042296  0.049415  0.024574 -0.023861  0.074412   \n",
       "17   0.070769  0.050680  0.012117  0.056301  0.034206  0.049416 -0.039719   \n",
       "18  -0.038207 -0.044642 -0.010517 -0.036656 -0.037344 -0.019476 -0.028674   \n",
       "19  -0.027310 -0.044642 -0.018062 -0.040099 -0.002945 -0.011335  0.037595   \n",
       "20  -0.049105 -0.044642 -0.056863 -0.043542 -0.045599 -0.043276  0.000779   \n",
       "21  -0.085430  0.050680 -0.022373  0.001215 -0.037344 -0.026366  0.015505   \n",
       "22  -0.085430 -0.044642 -0.004050 -0.009113 -0.002945  0.007767  0.022869   \n",
       "23   0.045341  0.050680  0.060618  0.031053  0.028702 -0.047347 -0.054446   \n",
       "24  -0.063635 -0.044642  0.035829 -0.022885 -0.030464 -0.018850 -0.006584   \n",
       "25  -0.067268  0.050680 -0.012673 -0.040099 -0.015328  0.004636 -0.058127   \n",
       "26  -0.107226 -0.044642 -0.077342 -0.026328 -0.089630 -0.096198  0.026550   \n",
       "27  -0.023677 -0.044642  0.059541 -0.040099 -0.042848 -0.043589  0.011824   \n",
       "28   0.052606 -0.044642 -0.021295 -0.074528 -0.040096 -0.037639 -0.006584   \n",
       "29   0.067136  0.050680 -0.006206  0.063187 -0.042848 -0.095885  0.052322   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "412  0.074401 -0.044642  0.085408  0.063187  0.014942  0.013091  0.015505   \n",
       "413 -0.052738 -0.044642 -0.000817 -0.026328  0.010815  0.007141  0.048640   \n",
       "414  0.081666  0.050680  0.006728 -0.004523  0.109883  0.117056 -0.032356   \n",
       "415 -0.005515 -0.044642  0.008883 -0.050428  0.025950  0.047224 -0.043401   \n",
       "416 -0.027310 -0.044642  0.080019  0.098763 -0.002945  0.018101 -0.017629   \n",
       "417 -0.052738 -0.044642  0.071397 -0.074528 -0.015328 -0.001314  0.004460   \n",
       "418  0.009016 -0.044642 -0.024529 -0.026328  0.098876  0.094196  0.070730   \n",
       "419 -0.020045 -0.044642 -0.054707 -0.053871 -0.066239 -0.057367  0.011824   \n",
       "420  0.023546 -0.044642 -0.036385  0.000068  0.001183  0.034698 -0.043401   \n",
       "421  0.038076  0.050680  0.016428  0.021872  0.039710  0.045032 -0.043401   \n",
       "422 -0.078165  0.050680  0.077863  0.052858  0.078236  0.064447  0.026550   \n",
       "423  0.009016  0.050680 -0.039618  0.028758  0.038334  0.073529 -0.072854   \n",
       "424  0.001751  0.050680  0.011039 -0.019442 -0.016704 -0.003819 -0.047082   \n",
       "425 -0.078165 -0.044642 -0.040696 -0.081414 -0.100638 -0.112795  0.022869   \n",
       "426  0.030811  0.050680 -0.034229  0.043677  0.057597  0.068831 -0.032356   \n",
       "427 -0.034575  0.050680  0.005650 -0.005671 -0.073119 -0.062691 -0.006584   \n",
       "428  0.048974  0.050680  0.088642  0.087287  0.035582  0.021546 -0.024993   \n",
       "429 -0.041840 -0.044642 -0.033151 -0.022885  0.046589  0.041587  0.056003   \n",
       "430 -0.009147 -0.044642 -0.056863 -0.050428  0.021822  0.045345 -0.028674   \n",
       "431  0.070769  0.050680 -0.030996  0.021872 -0.037344 -0.047034  0.033914   \n",
       "432  0.009016 -0.044642  0.055229 -0.005671  0.057597  0.044719 -0.002903   \n",
       "433 -0.027310 -0.044642 -0.060097 -0.029771  0.046589  0.019980  0.122273   \n",
       "434  0.016281 -0.044642  0.001339  0.008101  0.005311  0.010899  0.030232   \n",
       "435 -0.012780 -0.044642 -0.023451 -0.040099 -0.016704  0.004636 -0.017629   \n",
       "436 -0.056370 -0.044642 -0.074108 -0.050428 -0.024960 -0.047034  0.092820   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  \n",
       "0   -0.002592  0.019908 -0.017646  \n",
       "1   -0.039493 -0.068330 -0.092204  \n",
       "2   -0.002592  0.002864 -0.025930  \n",
       "3    0.034309  0.022692 -0.009362  \n",
       "4   -0.002592 -0.031991 -0.046641  \n",
       "5   -0.076395 -0.041180 -0.096346  \n",
       "6   -0.039493 -0.062913 -0.038357  \n",
       "7    0.017703 -0.035817  0.003064  \n",
       "8   -0.002592 -0.014956  0.011349  \n",
       "9   -0.002592  0.067736 -0.013504  \n",
       "10  -0.076395 -0.062913 -0.034215  \n",
       "11   0.071210 -0.096433 -0.059067  \n",
       "12  -0.039493 -0.030751 -0.042499  \n",
       "13  -0.002592  0.038393 -0.013504  \n",
       "14  -0.039493 -0.031991 -0.075636  \n",
       "15   0.108111  0.036056 -0.042499  \n",
       "16  -0.039493  0.052280  0.027917  \n",
       "17   0.034309  0.027368 -0.001078  \n",
       "18  -0.002592 -0.018118 -0.017646  \n",
       "19  -0.039493 -0.008944 -0.054925  \n",
       "20  -0.039493 -0.011901  0.015491  \n",
       "21  -0.039493 -0.072128 -0.017646  \n",
       "22  -0.039493 -0.061177 -0.013504  \n",
       "23   0.071210  0.133599  0.135612  \n",
       "24  -0.002592 -0.025952 -0.054925  \n",
       "25   0.034309  0.019199 -0.034215  \n",
       "26  -0.076395 -0.042572 -0.005220  \n",
       "27  -0.039493 -0.015998  0.040343  \n",
       "28  -0.039493 -0.000609 -0.054925  \n",
       "29  -0.076395  0.059424  0.052770  \n",
       "..        ...       ...       ...  \n",
       "412 -0.002592  0.006209  0.085907  \n",
       "413 -0.039493 -0.035817  0.019633  \n",
       "414  0.091875  0.054724  0.007207  \n",
       "415  0.071210  0.014823  0.003064  \n",
       "416  0.003312 -0.029528  0.036201  \n",
       "417 -0.021412 -0.046879  0.003064  \n",
       "418 -0.002592 -0.021394  0.007207  \n",
       "419 -0.039493 -0.074089 -0.005220  \n",
       "420  0.034309 -0.033249  0.061054  \n",
       "421  0.071210  0.049769  0.015491  \n",
       "422 -0.002592  0.040672 -0.009362  \n",
       "423  0.108111  0.015567 -0.046641  \n",
       "424  0.034309  0.024053  0.023775  \n",
       "425 -0.076395 -0.020289 -0.050783  \n",
       "426  0.057557  0.035462  0.085907  \n",
       "427 -0.039493 -0.045421  0.032059  \n",
       "428  0.034309  0.066048  0.131470  \n",
       "429 -0.024733 -0.025952 -0.038357  \n",
       "430  0.034309 -0.009919 -0.017646  \n",
       "431 -0.039493 -0.014956 -0.001078  \n",
       "432  0.023239  0.055684  0.106617  \n",
       "433 -0.039493 -0.051401 -0.009362  \n",
       "434 -0.039493 -0.045421  0.032059  \n",
       "435 -0.002592 -0.038459 -0.038357  \n",
       "436 -0.076395 -0.061177 -0.046641  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018118  0.044485  \n",
       "439 -0.011080 -0.046879  0.015491  \n",
       "440  0.026560  0.044528 -0.025930  \n",
       "441 -0.039493 -0.004220  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input\n",
    "#here all the variables are in scaled format already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age    0\n",
      "sex    0\n",
      "bmi    0\n",
      "bp     0\n",
      "s1     0\n",
      "s2     0\n",
      "s3     0\n",
      "s4     0\n",
      "s5     0\n",
      "s6     0\n",
      "dtype: int64\n",
      "\n",
      "DiseaseProgression    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Lets check whether it contains any null values or not\n",
    "print(df_input.isnull().sum())\n",
    "print()\n",
    "print(df_target.isnull().sum())\n",
    "#from below output its clear that given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>DiseaseProgression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.173737</td>\n",
       "      <td>0.185085</td>\n",
       "      <td>0.335427</td>\n",
       "      <td>0.260061</td>\n",
       "      <td>0.219243</td>\n",
       "      <td>-0.075181</td>\n",
       "      <td>0.203841</td>\n",
       "      <td>0.270777</td>\n",
       "      <td>0.301731</td>\n",
       "      <td>0.187889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.173737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.088161</td>\n",
       "      <td>0.241013</td>\n",
       "      <td>0.035277</td>\n",
       "      <td>0.142637</td>\n",
       "      <td>-0.379090</td>\n",
       "      <td>0.332115</td>\n",
       "      <td>0.149918</td>\n",
       "      <td>0.208133</td>\n",
       "      <td>0.043062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>0.185085</td>\n",
       "      <td>0.088161</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.395415</td>\n",
       "      <td>0.249777</td>\n",
       "      <td>0.261170</td>\n",
       "      <td>-0.366811</td>\n",
       "      <td>0.413807</td>\n",
       "      <td>0.446159</td>\n",
       "      <td>0.388680</td>\n",
       "      <td>0.586450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>0.335427</td>\n",
       "      <td>0.241013</td>\n",
       "      <td>0.395415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.242470</td>\n",
       "      <td>0.185558</td>\n",
       "      <td>-0.178761</td>\n",
       "      <td>0.257653</td>\n",
       "      <td>0.393478</td>\n",
       "      <td>0.390429</td>\n",
       "      <td>0.441484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s1</th>\n",
       "      <td>0.260061</td>\n",
       "      <td>0.035277</td>\n",
       "      <td>0.249777</td>\n",
       "      <td>0.242470</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.896663</td>\n",
       "      <td>0.051519</td>\n",
       "      <td>0.542207</td>\n",
       "      <td>0.515501</td>\n",
       "      <td>0.325717</td>\n",
       "      <td>0.212022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2</th>\n",
       "      <td>0.219243</td>\n",
       "      <td>0.142637</td>\n",
       "      <td>0.261170</td>\n",
       "      <td>0.185558</td>\n",
       "      <td>0.896663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.196455</td>\n",
       "      <td>0.659817</td>\n",
       "      <td>0.318353</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>0.174054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s3</th>\n",
       "      <td>-0.075181</td>\n",
       "      <td>-0.379090</td>\n",
       "      <td>-0.366811</td>\n",
       "      <td>-0.178761</td>\n",
       "      <td>0.051519</td>\n",
       "      <td>-0.196455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.738493</td>\n",
       "      <td>-0.398577</td>\n",
       "      <td>-0.273697</td>\n",
       "      <td>-0.394789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s4</th>\n",
       "      <td>0.203841</td>\n",
       "      <td>0.332115</td>\n",
       "      <td>0.413807</td>\n",
       "      <td>0.257653</td>\n",
       "      <td>0.542207</td>\n",
       "      <td>0.659817</td>\n",
       "      <td>-0.738493</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.617857</td>\n",
       "      <td>0.417212</td>\n",
       "      <td>0.430453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s5</th>\n",
       "      <td>0.270777</td>\n",
       "      <td>0.149918</td>\n",
       "      <td>0.446159</td>\n",
       "      <td>0.393478</td>\n",
       "      <td>0.515501</td>\n",
       "      <td>0.318353</td>\n",
       "      <td>-0.398577</td>\n",
       "      <td>0.617857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.464670</td>\n",
       "      <td>0.565883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s6</th>\n",
       "      <td>0.301731</td>\n",
       "      <td>0.208133</td>\n",
       "      <td>0.388680</td>\n",
       "      <td>0.390429</td>\n",
       "      <td>0.325717</td>\n",
       "      <td>0.290600</td>\n",
       "      <td>-0.273697</td>\n",
       "      <td>0.417212</td>\n",
       "      <td>0.464670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.382483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiseaseProgression</th>\n",
       "      <td>0.187889</td>\n",
       "      <td>0.043062</td>\n",
       "      <td>0.586450</td>\n",
       "      <td>0.441484</td>\n",
       "      <td>0.212022</td>\n",
       "      <td>0.174054</td>\n",
       "      <td>-0.394789</td>\n",
       "      <td>0.430453</td>\n",
       "      <td>0.565883</td>\n",
       "      <td>0.382483</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         age       sex       bmi        bp        s1  \\\n",
       "age                 1.000000  0.173737  0.185085  0.335427  0.260061   \n",
       "sex                 0.173737  1.000000  0.088161  0.241013  0.035277   \n",
       "bmi                 0.185085  0.088161  1.000000  0.395415  0.249777   \n",
       "bp                  0.335427  0.241013  0.395415  1.000000  0.242470   \n",
       "s1                  0.260061  0.035277  0.249777  0.242470  1.000000   \n",
       "s2                  0.219243  0.142637  0.261170  0.185558  0.896663   \n",
       "s3                 -0.075181 -0.379090 -0.366811 -0.178761  0.051519   \n",
       "s4                  0.203841  0.332115  0.413807  0.257653  0.542207   \n",
       "s5                  0.270777  0.149918  0.446159  0.393478  0.515501   \n",
       "s6                  0.301731  0.208133  0.388680  0.390429  0.325717   \n",
       "DiseaseProgression  0.187889  0.043062  0.586450  0.441484  0.212022   \n",
       "\n",
       "                          s2        s3        s4        s5        s6  \\\n",
       "age                 0.219243 -0.075181  0.203841  0.270777  0.301731   \n",
       "sex                 0.142637 -0.379090  0.332115  0.149918  0.208133   \n",
       "bmi                 0.261170 -0.366811  0.413807  0.446159  0.388680   \n",
       "bp                  0.185558 -0.178761  0.257653  0.393478  0.390429   \n",
       "s1                  0.896663  0.051519  0.542207  0.515501  0.325717   \n",
       "s2                  1.000000 -0.196455  0.659817  0.318353  0.290600   \n",
       "s3                 -0.196455  1.000000 -0.738493 -0.398577 -0.273697   \n",
       "s4                  0.659817 -0.738493  1.000000  0.617857  0.417212   \n",
       "s5                  0.318353 -0.398577  0.617857  1.000000  0.464670   \n",
       "s6                  0.290600 -0.273697  0.417212  0.464670  1.000000   \n",
       "DiseaseProgression  0.174054 -0.394789  0.430453  0.565883  0.382483   \n",
       "\n",
       "                    DiseaseProgression  \n",
       "age                           0.187889  \n",
       "sex                           0.043062  \n",
       "bmi                           0.586450  \n",
       "bp                            0.441484  \n",
       "s1                            0.212022  \n",
       "s2                            0.174054  \n",
       "s3                           -0.394789  \n",
       "s4                            0.430453  \n",
       "s5                            0.565883  \n",
       "s6                            0.382483  \n",
       "DiseaseProgression            1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check the correlation matrix\n",
    "pd.concat([df_input,df_target],axis=1).corr()\n",
    "#from below it visible that most of the features have week correlation with target and only some are around 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>DiseaseProgression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>4.420000e+02</td>\n",
       "      <td>442.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.634285e-16</td>\n",
       "      <td>1.308343e-16</td>\n",
       "      <td>-8.045349e-16</td>\n",
       "      <td>1.281655e-16</td>\n",
       "      <td>-8.835316e-17</td>\n",
       "      <td>1.327024e-16</td>\n",
       "      <td>-4.574646e-16</td>\n",
       "      <td>3.777301e-16</td>\n",
       "      <td>-3.830854e-16</td>\n",
       "      <td>-3.412882e-16</td>\n",
       "      <td>152.133484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>77.093005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.072256e-01</td>\n",
       "      <td>-4.464164e-02</td>\n",
       "      <td>-9.027530e-02</td>\n",
       "      <td>-1.123996e-01</td>\n",
       "      <td>-1.267807e-01</td>\n",
       "      <td>-1.156131e-01</td>\n",
       "      <td>-1.023071e-01</td>\n",
       "      <td>-7.639450e-02</td>\n",
       "      <td>-1.260974e-01</td>\n",
       "      <td>-1.377672e-01</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3.729927e-02</td>\n",
       "      <td>-4.464164e-02</td>\n",
       "      <td>-3.422907e-02</td>\n",
       "      <td>-3.665645e-02</td>\n",
       "      <td>-3.424784e-02</td>\n",
       "      <td>-3.035840e-02</td>\n",
       "      <td>-3.511716e-02</td>\n",
       "      <td>-3.949338e-02</td>\n",
       "      <td>-3.324879e-02</td>\n",
       "      <td>-3.317903e-02</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.383060e-03</td>\n",
       "      <td>-4.464164e-02</td>\n",
       "      <td>-7.283766e-03</td>\n",
       "      <td>-5.670611e-03</td>\n",
       "      <td>-4.320866e-03</td>\n",
       "      <td>-3.819065e-03</td>\n",
       "      <td>-6.584468e-03</td>\n",
       "      <td>-2.592262e-03</td>\n",
       "      <td>-1.947634e-03</td>\n",
       "      <td>-1.077698e-03</td>\n",
       "      <td>140.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.807591e-02</td>\n",
       "      <td>5.068012e-02</td>\n",
       "      <td>3.124802e-02</td>\n",
       "      <td>3.564384e-02</td>\n",
       "      <td>2.835801e-02</td>\n",
       "      <td>2.984439e-02</td>\n",
       "      <td>2.931150e-02</td>\n",
       "      <td>3.430886e-02</td>\n",
       "      <td>3.243323e-02</td>\n",
       "      <td>2.791705e-02</td>\n",
       "      <td>211.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.107267e-01</td>\n",
       "      <td>5.068012e-02</td>\n",
       "      <td>1.705552e-01</td>\n",
       "      <td>1.320442e-01</td>\n",
       "      <td>1.539137e-01</td>\n",
       "      <td>1.987880e-01</td>\n",
       "      <td>1.811791e-01</td>\n",
       "      <td>1.852344e-01</td>\n",
       "      <td>1.335990e-01</td>\n",
       "      <td>1.356118e-01</td>\n",
       "      <td>346.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age           sex           bmi            bp            s1  \\\n",
       "count  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02   \n",
       "mean  -3.634285e-16  1.308343e-16 -8.045349e-16  1.281655e-16 -8.835316e-17   \n",
       "std    4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02   \n",
       "min   -1.072256e-01 -4.464164e-02 -9.027530e-02 -1.123996e-01 -1.267807e-01   \n",
       "25%   -3.729927e-02 -4.464164e-02 -3.422907e-02 -3.665645e-02 -3.424784e-02   \n",
       "50%    5.383060e-03 -4.464164e-02 -7.283766e-03 -5.670611e-03 -4.320866e-03   \n",
       "75%    3.807591e-02  5.068012e-02  3.124802e-02  3.564384e-02  2.835801e-02   \n",
       "max    1.107267e-01  5.068012e-02  1.705552e-01  1.320442e-01  1.539137e-01   \n",
       "\n",
       "                 s2            s3            s4            s5            s6  \\\n",
       "count  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02  4.420000e+02   \n",
       "mean   1.327024e-16 -4.574646e-16  3.777301e-16 -3.830854e-16 -3.412882e-16   \n",
       "std    4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02  4.761905e-02   \n",
       "min   -1.156131e-01 -1.023071e-01 -7.639450e-02 -1.260974e-01 -1.377672e-01   \n",
       "25%   -3.035840e-02 -3.511716e-02 -3.949338e-02 -3.324879e-02 -3.317903e-02   \n",
       "50%   -3.819065e-03 -6.584468e-03 -2.592262e-03 -1.947634e-03 -1.077698e-03   \n",
       "75%    2.984439e-02  2.931150e-02  3.430886e-02  3.243323e-02  2.791705e-02   \n",
       "max    1.987880e-01  1.811791e-01  1.852344e-01  1.335990e-01  1.356118e-01   \n",
       "\n",
       "       DiseaseProgression  \n",
       "count          442.000000  \n",
       "mean           152.133484  \n",
       "std             77.093005  \n",
       "min             25.000000  \n",
       "25%             87.000000  \n",
       "50%            140.500000  \n",
       "75%            211.500000  \n",
       "max            346.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets describe the dataset\n",
    "pd.concat([df_input,df_target],axis=1).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(430, 11)\n"
     ]
    }
   ],
   "source": [
    "#lets check for outliers using z score\n",
    "from scipy.stats import zscore\n",
    "df_combined=pd.concat([df_input,df_target],axis=1)\n",
    "z_scr=zscore(df_combined)\n",
    "df_combined_new=df_combined.loc[(abs(z_scr)<3).all(axis=1)]\n",
    "print(df_combined_new.shape)\n",
    "#from below output it is clear that some outliers were there and now those are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seprating input and output from df_combined_new\n",
    "df_x=df_combined_new.drop(columns=[\"DiseaseProgression\"])\n",
    "y=df_combined_new[[\"DiseaseProgression\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "def maxr2_score(regr,df_x,y):\n",
    "    max_r_score=0\n",
    "    for r_state in range(42,100):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x, y,random_state = r_state,test_size=0.20)\n",
    "        regr.fit(x_train,y_train)\n",
    "        y_pred = regr.predict(x_test)\n",
    "        r2_scr=r2_score(y_test,y_pred)\n",
    "        print(\"r2 score corresponding to \",r_state,\" is \",r2_scr)\n",
    "        if r2_scr>max_r_score:\n",
    "            max_r_score=r2_scr\n",
    "            final_r_state=r_state\n",
    "    print(\"max r2 score corresponding to \",final_r_state,\" is \",max_r_score)\n",
    "    return final_r_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to  42  is  0.3666696589544701\n",
      "r2 score corresponding to  43  is  0.5414633719716159\n",
      "r2 score corresponding to  44  is  0.502830202394811\n",
      "r2 score corresponding to  45  is  0.41940208977825444\n",
      "r2 score corresponding to  46  is  0.49944594921344987\n",
      "r2 score corresponding to  47  is  0.5696168464053883\n",
      "r2 score corresponding to  48  is  0.48941614789926113\n",
      "r2 score corresponding to  49  is  0.4157614115642505\n",
      "r2 score corresponding to  50  is  0.5113404267738513\n",
      "r2 score corresponding to  51  is  0.25353904234282654\n",
      "r2 score corresponding to  52  is  0.6129709896007236\n",
      "r2 score corresponding to  53  is  0.41643342507750947\n",
      "r2 score corresponding to  54  is  0.5374683464284008\n",
      "r2 score corresponding to  55  is  0.48857235252489817\n",
      "r2 score corresponding to  56  is  0.31394754546793013\n",
      "r2 score corresponding to  57  is  0.3987795821170861\n",
      "r2 score corresponding to  58  is  0.5386412864690377\n",
      "r2 score corresponding to  59  is  0.4399904821531171\n",
      "r2 score corresponding to  60  is  0.47232409291236543\n",
      "r2 score corresponding to  61  is  0.603076673940736\n",
      "r2 score corresponding to  62  is  0.32857273778439056\n",
      "r2 score corresponding to  63  is  0.3709776124440525\n",
      "r2 score corresponding to  64  is  0.4781873297146566\n",
      "r2 score corresponding to  65  is  0.42403073307148265\n",
      "r2 score corresponding to  66  is  0.34523135968037644\n",
      "r2 score corresponding to  67  is  0.5211022173934445\n",
      "r2 score corresponding to  68  is  0.4660328632026337\n",
      "r2 score corresponding to  69  is  0.5993215324789298\n",
      "r2 score corresponding to  70  is  0.5891081514574839\n",
      "r2 score corresponding to  71  is  0.4725769890003353\n",
      "r2 score corresponding to  72  is  0.465070394091211\n",
      "r2 score corresponding to  73  is  0.4516693884290638\n",
      "r2 score corresponding to  74  is  0.5042728258045484\n",
      "r2 score corresponding to  75  is  0.47811967985500126\n",
      "r2 score corresponding to  76  is  0.45568279714616433\n",
      "r2 score corresponding to  77  is  0.5085873212683414\n",
      "r2 score corresponding to  78  is  0.36407345814686276\n",
      "r2 score corresponding to  79  is  0.3725541958099977\n",
      "r2 score corresponding to  80  is  0.5236724939758377\n",
      "r2 score corresponding to  81  is  0.4253354835748894\n",
      "r2 score corresponding to  82  is  0.31840079720460923\n",
      "r2 score corresponding to  83  is  0.39070077789759694\n",
      "r2 score corresponding to  84  is  0.5327667583770852\n",
      "r2 score corresponding to  85  is  0.4276220530690753\n",
      "r2 score corresponding to  86  is  0.5115984646910832\n",
      "r2 score corresponding to  87  is  0.4723584343817828\n",
      "r2 score corresponding to  88  is  0.46325086683481786\n",
      "r2 score corresponding to  89  is  0.4701807561449324\n",
      "r2 score corresponding to  90  is  0.42789249433583443\n",
      "r2 score corresponding to  91  is  0.5349020780910557\n",
      "r2 score corresponding to  92  is  0.5351907235215829\n",
      "r2 score corresponding to  93  is  0.43052728405482854\n",
      "r2 score corresponding to  94  is  0.46589277598171586\n",
      "r2 score corresponding to  95  is  0.4276807654980973\n",
      "r2 score corresponding to  96  is  0.48252164426074473\n",
      "r2 score corresponding to  97  is  0.5450125036671557\n",
      "r2 score corresponding to  98  is  0.43828298121112075\n",
      "r2 score corresponding to  99  is  0.626951653171429\n",
      "max r2 score corresponding to  99  is  0.626951653171429\n"
     ]
    }
   ],
   "source": [
    "#Lets use Linear regression and chcek max r2 score corresponding to different random states\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lreg=LinearRegression()\n",
    "r_state=maxr2_score(lreg,df_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 19}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets use grid_search to find optimal value of n_neigbors for KNN model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neighbors={\"n_neighbors\":range(1,30)}\n",
    "knr=KNeighborsRegressor()\n",
    "gknr = GridSearchCV(knr, neighbors, cv=10)\n",
    "gknr.fit(df_x,y)\n",
    "gknr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to  42  is  0.3754639421781797\n",
      "r2 score corresponding to  43  is  0.4850176749925311\n",
      "r2 score corresponding to  44  is  0.5070124907462026\n",
      "r2 score corresponding to  45  is  0.34696634001528726\n",
      "r2 score corresponding to  46  is  0.451542905441379\n",
      "r2 score corresponding to  47  is  0.4895196971602195\n",
      "r2 score corresponding to  48  is  0.4958809239179024\n",
      "r2 score corresponding to  49  is  0.42630690730410414\n",
      "r2 score corresponding to  50  is  0.45313119612132746\n",
      "r2 score corresponding to  51  is  0.2678001378172621\n",
      "r2 score corresponding to  52  is  0.5358847489904657\n",
      "r2 score corresponding to  53  is  0.4586727843601762\n",
      "r2 score corresponding to  54  is  0.46474045120456287\n",
      "r2 score corresponding to  55  is  0.41518128514327657\n",
      "r2 score corresponding to  56  is  0.33146594793420947\n",
      "r2 score corresponding to  57  is  0.41031633084078845\n",
      "r2 score corresponding to  58  is  0.5402584419605948\n",
      "r2 score corresponding to  59  is  0.42953599823241373\n",
      "r2 score corresponding to  60  is  0.46738745945600657\n",
      "r2 score corresponding to  61  is  0.5553079262704815\n",
      "r2 score corresponding to  62  is  0.33066066702607766\n",
      "r2 score corresponding to  63  is  0.3783895137967106\n",
      "r2 score corresponding to  64  is  0.48421379326260394\n",
      "r2 score corresponding to  65  is  0.44261746471618224\n",
      "r2 score corresponding to  66  is  0.2698653959802415\n",
      "r2 score corresponding to  67  is  0.36908453542069697\n",
      "r2 score corresponding to  68  is  0.4394873343265039\n",
      "r2 score corresponding to  69  is  0.5187953839385533\n",
      "r2 score corresponding to  70  is  0.5208596450734551\n",
      "r2 score corresponding to  71  is  0.4167489822309206\n",
      "r2 score corresponding to  72  is  0.39331922475442993\n",
      "r2 score corresponding to  73  is  0.43251087373979524\n",
      "r2 score corresponding to  74  is  0.5170866920482625\n",
      "r2 score corresponding to  75  is  0.4478426348533341\n",
      "r2 score corresponding to  76  is  0.4339141161652411\n",
      "r2 score corresponding to  77  is  0.41530671180657985\n",
      "r2 score corresponding to  78  is  0.42243925700111273\n",
      "r2 score corresponding to  79  is  0.3505917019162451\n",
      "r2 score corresponding to  80  is  0.4245561118826796\n",
      "r2 score corresponding to  81  is  0.34241234907433316\n",
      "r2 score corresponding to  82  is  0.38038532749795073\n",
      "r2 score corresponding to  83  is  0.397615256396471\n",
      "r2 score corresponding to  84  is  0.4114107020200227\n",
      "r2 score corresponding to  85  is  0.4874359343901714\n",
      "r2 score corresponding to  86  is  0.471843473823647\n",
      "r2 score corresponding to  87  is  0.475164354198919\n",
      "r2 score corresponding to  88  is  0.37825637663616063\n",
      "r2 score corresponding to  89  is  0.4121324563017762\n",
      "r2 score corresponding to  90  is  0.43964228007857065\n",
      "r2 score corresponding to  91  is  0.4851635167156674\n",
      "r2 score corresponding to  92  is  0.4902993334212776\n",
      "r2 score corresponding to  93  is  0.41204732125202204\n",
      "r2 score corresponding to  94  is  0.447939845627653\n",
      "r2 score corresponding to  95  is  0.372989497860159\n",
      "r2 score corresponding to  96  is  0.4798458335710769\n",
      "r2 score corresponding to  97  is  0.4558562085206812\n",
      "r2 score corresponding to  98  is  0.4429255304005809\n",
      "r2 score corresponding to  99  is  0.5372893665697835\n",
      "max r2 score corresponding to  61  is  0.5553079262704815\n"
     ]
    }
   ],
   "source": [
    "#Lets use KNN regression and chcek max r2 score corresponding to different random states\n",
    "knr=KNeighborsRegressor(n_neighbors=19)\n",
    "r_state=maxr2_score(knr,df_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for Linear Regression:  0.4676911221693185\n",
      "standard deviation in r2 score for Linear Regression:  0.05595217785841939\n",
      "\n",
      "Mean r2 score for KNN Regression:  0.4447393304649888\n",
      "standard deviation in r2 score for KNN Regression:  0.06885364198445655\n"
     ]
    }
   ],
   "source": [
    "#Lets check the mean r2 score of both linear regression model and knn regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(\"Mean r2 score for Linear Regression: \",cross_val_score(lreg,df_x,y,cv=5,scoring=\"r2\").mean())\n",
    "print(\"standard deviation in r2 score for Linear Regression: \",cross_val_score(lreg,df_x,y,cv=5,scoring=\"r2\").std())\n",
    "print()\n",
    "print(\"Mean r2 score for KNN Regression: \",cross_val_score(knr,df_x,y,cv=5,scoring=\"r2\").mean())\n",
    "print(\"standard deviation in r2 score for KNN Regression: \",cross_val_score(knr,df_x,y,cv=5,scoring=\"r2\").std())\n",
    "#Based on below output linear regression is performing well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check lasso regression and find best value of alpha\n",
    "from sklearn.linear_model import Lasso\n",
    "lsreg=Lasso()\n",
    "parameters={\"alpha\":[0.001,0.01,0.1,1]}\n",
    "clf = GridSearchCV(lsreg, parameters, cv=10)\n",
    "clf.fit(df_x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to  42  is  0.39559614133403775\n",
      "r2 score corresponding to  43  is  0.5279704191720399\n",
      "r2 score corresponding to  44  is  0.50503294565865\n",
      "r2 score corresponding to  45  is  0.4036196797761409\n",
      "r2 score corresponding to  46  is  0.5035821455123144\n",
      "r2 score corresponding to  47  is  0.5557723339863485\n",
      "r2 score corresponding to  48  is  0.4951902457271492\n",
      "r2 score corresponding to  49  is  0.41898867700436127\n",
      "r2 score corresponding to  50  is  0.4841336844871694\n",
      "r2 score corresponding to  51  is  0.2705971586987356\n",
      "r2 score corresponding to  52  is  0.5965279705889219\n",
      "r2 score corresponding to  53  is  0.4227011451196119\n",
      "r2 score corresponding to  54  is  0.514897039321015\n",
      "r2 score corresponding to  55  is  0.4921979652112449\n",
      "r2 score corresponding to  56  is  0.3249514143291744\n",
      "r2 score corresponding to  57  is  0.41824855781541936\n",
      "r2 score corresponding to  58  is  0.5438396084200854\n",
      "r2 score corresponding to  59  is  0.44859182287998944\n",
      "r2 score corresponding to  60  is  0.4998521969434333\n",
      "r2 score corresponding to  61  is  0.5946436513155507\n",
      "r2 score corresponding to  62  is  0.3292015646228238\n",
      "r2 score corresponding to  63  is  0.366514810681454\n",
      "r2 score corresponding to  64  is  0.4886841889561808\n",
      "r2 score corresponding to  65  is  0.4243759279884286\n",
      "r2 score corresponding to  66  is  0.3558024076403584\n",
      "r2 score corresponding to  67  is  0.49659168019338973\n",
      "r2 score corresponding to  68  is  0.4610847398975164\n",
      "r2 score corresponding to  69  is  0.5728238404592398\n",
      "r2 score corresponding to  70  is  0.5743121261219555\n",
      "r2 score corresponding to  71  is  0.449548578170593\n",
      "r2 score corresponding to  72  is  0.4629130249893435\n",
      "r2 score corresponding to  73  is  0.46881687014165185\n",
      "r2 score corresponding to  74  is  0.4959419828959617\n",
      "r2 score corresponding to  75  is  0.4797666406182508\n",
      "r2 score corresponding to  76  is  0.4554730925472029\n",
      "r2 score corresponding to  77  is  0.4794757306005858\n",
      "r2 score corresponding to  78  is  0.36964302184106534\n",
      "r2 score corresponding to  79  is  0.37585626273279515\n",
      "r2 score corresponding to  80  is  0.5104689802704394\n",
      "r2 score corresponding to  81  is  0.4217938349296846\n",
      "r2 score corresponding to  82  is  0.3899368754484932\n",
      "r2 score corresponding to  83  is  0.40682839935367054\n",
      "r2 score corresponding to  84  is  0.5065218361430214\n",
      "r2 score corresponding to  85  is  0.44823689701050684\n",
      "r2 score corresponding to  86  is  0.5100453740518435\n",
      "r2 score corresponding to  87  is  0.4680572800564162\n",
      "r2 score corresponding to  88  is  0.458121294826466\n",
      "r2 score corresponding to  89  is  0.47086258108514245\n",
      "r2 score corresponding to  90  is  0.4115773586895217\n",
      "r2 score corresponding to  91  is  0.5272547222803922\n",
      "r2 score corresponding to  92  is  0.5220636016263058\n",
      "r2 score corresponding to  93  is  0.43594244047480246\n",
      "r2 score corresponding to  94  is  0.4822746405544437\n",
      "r2 score corresponding to  95  is  0.40475836442100754\n",
      "r2 score corresponding to  96  is  0.4807202490141723\n",
      "r2 score corresponding to  97  is  0.5373235191356918\n",
      "r2 score corresponding to  98  is  0.4364630009575615\n",
      "r2 score corresponding to  99  is  0.5997275247178083\n",
      "max r2 score corresponding to  99  is  0.5997275247178083\n"
     ]
    }
   ],
   "source": [
    "#Lets check max r2 score when we use lasso\n",
    "lsreg=Lasso(alpha=0.1)\n",
    "r_state=maxr2_score(lsreg,df_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for Lasso Regression:  0.4668358456202258\n",
      "standard deviation in r2 score for Lasso Regression:  0.05024883603439513\n"
     ]
    }
   ],
   "source": [
    "#Lets use cross val score with Lasso\n",
    "print(\"Mean r2 score for Lasso Regression: \",cross_val_score(lsreg,df_x,y,cv=5,scoring=\"r2\").mean())\n",
    "print(\"standard deviation in r2 score for Lasso Regression: \",cross_val_score(lsreg,df_x,y,cv=5,scoring=\"r2\").std())\n",
    "#Based on below output i can say Lasso perfored just like linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01, 'n_estimators': 500}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we will try to use gradient boosting technique\n",
    "#For getting best set of parmeters we will use grid search\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "gbr=GradientBoostingRegressor()\n",
    "parameters={\"learning_rate\":[0.001,0.01,0.1,1],\"n_estimators\":[10,100,500,1000]}\n",
    "clf = GridSearchCV(gbr, parameters, cv=5)\n",
    "clf.fit(df_x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for gradient boosting Regression:  0.4082002424361124\n",
      "standard deviation in r2 score for gradient boosting Regression:  0.07586530903095573\n"
     ]
    }
   ],
   "source": [
    "#Now we will use cross val score to check the mean r2 score and standard deviation\n",
    "gbr=GradientBoostingRegressor(learning_rate=0.01,n_estimators=500)\n",
    "print(\"Mean r2 score for gradient boosting Regression: \",cross_val_score(gbr,df_x,y,cv=5,scoring=\"r2\").mean())\n",
    "print(\"standard deviation in r2 score for gradient boosting Regression: \",cross_val_score(gbr,df_x,y,cv=5,scoring=\"r2\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "    normalize=False, positive=False, precompute=False, random_state=None,\n",
       "    selection='cyclic', tol=0.0001, warm_start=False),\n",
       " 'learning_rate': 0.1,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now lets use ada boost regression alagorithm\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "ada_reg=AdaBoostRegressor()\n",
    "parameters={\"learning_rate\":[0.001,0.01,0.1,1],\"n_estimators\":[10,100,500,1000],\"base_estimator\":[lreg,lsreg,DecisionTreeRegressor()]}\n",
    "clf = GridSearchCV(ada_reg, parameters, cv=5)\n",
    "clf.fit(df_x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for ada boosting Regression:  0.4618571878101784\n",
      "standard deviation in r2 score for ada boosting Regression:  0.05195634581197533\n"
     ]
    }
   ],
   "source": [
    "ada_reg=AdaBoostRegressor(base_estimator=lsreg,learning_rate=0.001,n_estimators=10)\n",
    "print(\"Mean r2 score for ada boosting Regression: \",cross_val_score(ada_reg,df_x,y,cv=5,scoring=\"r2\").mean())\n",
    "print(\"standard deviation in r2 score for ada boosting Regression: \",cross_val_score(ada_reg,df_x,y,cv=5,scoring=\"r2\").std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to  42  is  0.3806311885890201\n",
      "r2 score corresponding to  43  is  0.5217300457546591\n",
      "r2 score corresponding to  44  is  0.49399684374068553\n",
      "r2 score corresponding to  45  is  0.3968265775291141\n",
      "r2 score corresponding to  46  is  0.4941221143289968\n",
      "r2 score corresponding to  47  is  0.5480471782996068\n",
      "r2 score corresponding to  48  is  0.48737847603852924\n",
      "r2 score corresponding to  49  is  0.4248305670591346\n",
      "r2 score corresponding to  50  is  0.48590495743488193\n",
      "r2 score corresponding to  51  is  0.2678589808316927\n",
      "r2 score corresponding to  52  is  0.5962712405219497\n",
      "r2 score corresponding to  53  is  0.4203638412380676\n",
      "r2 score corresponding to  54  is  0.5203147678635328\n",
      "r2 score corresponding to  55  is  0.4899418790253677\n",
      "r2 score corresponding to  56  is  0.3316427676451109\n",
      "r2 score corresponding to  57  is  0.4232445098905103\n",
      "r2 score corresponding to  58  is  0.5374387024940226\n",
      "r2 score corresponding to  59  is  0.4323262861262481\n",
      "r2 score corresponding to  60  is  0.502827827303095\n",
      "r2 score corresponding to  61  is  0.5894441464903647\n",
      "r2 score corresponding to  62  is  0.3180868979446426\n",
      "r2 score corresponding to  63  is  0.3712270568677326\n",
      "r2 score corresponding to  64  is  0.4820083062028594\n",
      "r2 score corresponding to  65  is  0.41102413972935614\n",
      "r2 score corresponding to  66  is  0.3556616121339168\n",
      "r2 score corresponding to  67  is  0.49496817880887745\n",
      "r2 score corresponding to  68  is  0.4648081458107969\n",
      "r2 score corresponding to  69  is  0.5623044335860266\n",
      "r2 score corresponding to  70  is  0.5623766224485606\n",
      "r2 score corresponding to  71  is  0.45924351853840517\n",
      "r2 score corresponding to  72  is  0.461814713083865\n",
      "r2 score corresponding to  73  is  0.46514816767586\n",
      "r2 score corresponding to  74  is  0.49055824966379713\n",
      "r2 score corresponding to  75  is  0.478204503773502\n",
      "r2 score corresponding to  76  is  0.4525790404279538\n",
      "r2 score corresponding to  77  is  0.48431266882043544\n",
      "r2 score corresponding to  78  is  0.37497910586396055\n",
      "r2 score corresponding to  79  is  0.3769756389469727\n",
      "r2 score corresponding to  80  is  0.5050547846015878\n",
      "r2 score corresponding to  81  is  0.41493889117941984\n",
      "r2 score corresponding to  82  is  0.40083892618992034\n",
      "r2 score corresponding to  83  is  0.4082604365600587\n",
      "r2 score corresponding to  84  is  0.4971164064006728\n",
      "r2 score corresponding to  85  is  0.4396767641536379\n",
      "r2 score corresponding to  86  is  0.5005519105394247\n",
      "r2 score corresponding to  87  is  0.4730633688630934\n",
      "r2 score corresponding to  88  is  0.46231061366760007\n",
      "r2 score corresponding to  89  is  0.4657100852165751\n",
      "r2 score corresponding to  90  is  0.4111201928420649\n",
      "r2 score corresponding to  91  is  0.520369770068494\n",
      "r2 score corresponding to  92  is  0.518275888985297\n",
      "r2 score corresponding to  93  is  0.4243639617120758\n",
      "r2 score corresponding to  94  is  0.4860325367926146\n",
      "r2 score corresponding to  95  is  0.41244851558744156\n",
      "r2 score corresponding to  96  is  0.4742701174620322\n",
      "r2 score corresponding to  97  is  0.5246441839964662\n",
      "r2 score corresponding to  98  is  0.4357180111792245\n",
      "r2 score corresponding to  99  is  0.5986015107485738\n",
      "max r2 score corresponding to  99  is  0.5986015107485738\n"
     ]
    }
   ],
   "source": [
    "#lets check maximum r2_score corresponding to this\n",
    "r_state=maxr2_score(ada_reg,df_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we tried all the models and till now linear regression is the best\n",
    "#random state corresponding to highest r2_score is 99\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, y,random_state = 99,test_size=0.20)\n",
    "lreg=LinearRegression()\n",
    "lreg.fit(x_train,y_train)\n",
    "y_pred=lreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is:  48.95593018443329\n",
      "r2_score is:  0.626951653171429\n"
     ]
    }
   ],
   "source": [
    "#Lets find the rmse and r2_score using sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE is: \",np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "print(\"r2_score is: \",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diabetes_prog_lreg.pkl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib \n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(lreg, 'diabetes_prog_lreg.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

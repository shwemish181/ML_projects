{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>L</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>R</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>L</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>B</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>625 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2  3  4\n",
       "0    B  1  1  1  1\n",
       "1    R  1  1  1  2\n",
       "2    R  1  1  1  3\n",
       "3    R  1  1  1  4\n",
       "4    R  1  1  1  5\n",
       "5    R  1  1  2  1\n",
       "6    R  1  1  2  2\n",
       "7    R  1  1  2  3\n",
       "8    R  1  1  2  4\n",
       "9    R  1  1  2  5\n",
       "10   R  1  1  3  1\n",
       "11   R  1  1  3  2\n",
       "12   R  1  1  3  3\n",
       "13   R  1  1  3  4\n",
       "14   R  1  1  3  5\n",
       "15   R  1  1  4  1\n",
       "16   R  1  1  4  2\n",
       "17   R  1  1  4  3\n",
       "18   R  1  1  4  4\n",
       "19   R  1  1  4  5\n",
       "20   R  1  1  5  1\n",
       "21   R  1  1  5  2\n",
       "22   R  1  1  5  3\n",
       "23   R  1  1  5  4\n",
       "24   R  1  1  5  5\n",
       "25   L  1  2  1  1\n",
       "26   B  1  2  1  2\n",
       "27   R  1  2  1  3\n",
       "28   R  1  2  1  4\n",
       "29   R  1  2  1  5\n",
       "..  .. .. .. .. ..\n",
       "595  L  5  4  5  1\n",
       "596  L  5  4  5  2\n",
       "597  L  5  4  5  3\n",
       "598  B  5  4  5  4\n",
       "599  R  5  4  5  5\n",
       "600  L  5  5  1  1\n",
       "601  L  5  5  1  2\n",
       "602  L  5  5  1  3\n",
       "603  L  5  5  1  4\n",
       "604  L  5  5  1  5\n",
       "605  L  5  5  2  1\n",
       "606  L  5  5  2  2\n",
       "607  L  5  5  2  3\n",
       "608  L  5  5  2  4\n",
       "609  L  5  5  2  5\n",
       "610  L  5  5  3  1\n",
       "611  L  5  5  3  2\n",
       "612  L  5  5  3  3\n",
       "613  L  5  5  3  4\n",
       "614  L  5  5  3  5\n",
       "615  L  5  5  4  1\n",
       "616  L  5  5  4  2\n",
       "617  L  5  5  4  3\n",
       "618  L  5  5  4  4\n",
       "619  L  5  5  4  5\n",
       "620  L  5  5  5  1\n",
       "621  L  5  5  5  2\n",
       "622  L  5  5  5  3\n",
       "623  L  5  5  5  4\n",
       "624  B  5  5  5  5\n",
       "\n",
       "[625 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_balance=pd.read_csv(\"balance_scale.txt\",sep=\",\",header=None)\n",
    "df_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets provide column names to the dataframe\n",
    "df_balance.columns=[\"target\",\"att1\",\"att2\",\"att3\",\"att4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "att1      0\n",
       "att2      0\n",
       "att3      0\n",
       "att4      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check whether null values are present or not\n",
    "df_balance.isnull().sum()\n",
    "#there are no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "      <td>625.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.415346</td>\n",
       "      <td>1.415346</td>\n",
       "      <td>1.415346</td>\n",
       "      <td>1.415346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             att1        att2        att3        att4\n",
       "count  625.000000  625.000000  625.000000  625.000000\n",
       "mean     3.000000    3.000000    3.000000    3.000000\n",
       "std      1.415346    1.415346    1.415346    1.415346\n",
       "min      1.000000    1.000000    1.000000    1.000000\n",
       "25%      2.000000    2.000000    2.000000    2.000000\n",
       "50%      3.000000    3.000000    3.000000    3.000000\n",
       "75%      4.000000    4.000000    4.000000    4.000000\n",
       "max      5.000000    5.000000    5.000000    5.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets describe the dataframe\n",
    "df_balance.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>att1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>att2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>att3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>att4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      att1  att2  att3  att4\n",
       "att1   1.0   0.0   0.0   0.0\n",
       "att2   0.0   1.0   0.0   0.0\n",
       "att3   0.0   0.0   1.0   0.0\n",
       "att4   0.0   0.0   0.0   1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets chcek the correlation\n",
    "df_balance.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "att1    0.0\n",
       "att2    0.0\n",
       "att3    0.0\n",
       "att4    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets chcek the skewness of the dataset\n",
    "df_balance.skew()\n",
    "#there is no skewness at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets chcek the count of each type of target class\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x=\"target\",data=df_balance)\n",
    "plt.show()\n",
    "#count of B type is very less\n",
    "#L and R have equal counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets divide input and output\n",
    "df_x=df_balance.drop(columns=[\"target\"])\n",
    "y=df_balance[[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "F:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#let's bring the features to same scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "sc.fit(df_x)\n",
    "x=sc.transform(df_x)\n",
    "x=pd.DataFrame(x,columns=df_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "F:\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 1, 1, 0, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0, 2, 1, 0, 2, 2, 2, 1, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 2, 2,\n",
       "       1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 0, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 0,\n",
       "       2, 1, 0, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 2, 2, 1, 0, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2,\n",
       "       2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 2, 2, 2, 1, 0, 2, 2, 2,\n",
       "       1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1,\n",
       "       2, 2, 2, 1, 0, 2, 2, 2, 1, 1, 0, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2, 1,\n",
       "       0, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 2, 1, 1, 0, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 0, 2, 2, 1, 1, 2, 2, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 0, 2,\n",
       "       2, 1, 1, 1, 0, 2, 1, 0, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 2, 2, 2, 1, 0, 2,\n",
       "       2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2,\n",
       "       1, 1, 0, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 1,\n",
       "       1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 0, 2, 2, 2,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1,\n",
       "       0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets use labelencoder to convert target class into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "le.fit(y)\n",
    "y=le.transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since it is imbalanced dataset so we will focus on auc-roc score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "def max_aucroc_score(clf,df_x,y):\n",
    "    max_aucroc_score=0\n",
    "    for r_state in range(42,100):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(df_x, y,random_state = r_state,test_size=0.10,stratify=y)\n",
    "        x_train, y_train = SMOTE().fit_sample(x_train, y_train)\n",
    "        clf.fit(x_train,y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        aucroc_scr=roc_auc_score(y_test,y_pred)\n",
    "        print(\"auc roc score corresponding to \",r_state,\" is \",aucroc_scr)\n",
    "        if aucroc_scr>max_aucroc_score:\n",
    "            max_aucroc_score=aucroc_scr\n",
    "            final_r_state=r_state\n",
    "    print(\"max auc roc score corresponding to \",final_r_state,\" is \",max_aucroc_score)\n",
    "    return final_r_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets make a function which evaluates the model using cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def model_evaluation(model,x,y):\n",
    "    print(\"Mean roc auc score for classifier: \",cross_val_score(model,x,y,cv=10,scoring=\"roc_auc\").mean())\n",
    "    print(\"standard deviation in roc auc score for classifier: \",cross_val_score(model,x,y,cv=10,scoring=\"roc_auc\").std())\n",
    "    print(cross_val_score(model,x,y,cv=10,scoring=\"roc_auc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 25}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets chcek KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "kc=KNeighborsClassifier()\n",
    "neighbors={\"n_neighbors\":range(1,30)}\n",
    "clf = GridSearchCV(kc, neighbors, cv=10,scoring=\"roc_auc\")\n",
    "y1 = label_binarize(y, classes=[0, 1, 2])\n",
    "clf.fit(x,y1)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc roc score corresponding to  42  is  0.7591954022988506\n",
      "auc roc score corresponding to  43  is  0.7152129817444219\n",
      "auc roc score corresponding to  44  is  0.6721095334685598\n",
      "auc roc score corresponding to  45  is  0.7620689655172415\n",
      "auc roc score corresponding to  46  is  0.7419540229885057\n",
      "auc roc score corresponding to  47  is  0.7721095334685598\n",
      "auc roc score corresponding to  48  is  0.7427991886409737\n",
      "auc roc score corresponding to  49  is  0.7086206896551724\n",
      "auc roc score corresponding to  50  is  0.7649425287356322\n",
      "auc roc score corresponding to  51  is  0.7448275862068966\n",
      "auc roc score corresponding to  52  is  0.6971264367816091\n",
      "auc roc score corresponding to  53  is  0.7724137931034484\n",
      "auc roc score corresponding to  54  is  0.7160919540229885\n",
      "auc roc score corresponding to  55  is  0.767816091954023\n",
      "auc roc score corresponding to  56  is  0.8106152805949968\n",
      "auc roc score corresponding to  57  is  0.7649425287356321\n",
      "auc roc score corresponding to  58  is  0.7028735632183908\n",
      "auc roc score corresponding to  59  is  0.7407707910750506\n",
      "auc roc score corresponding to  60  is  0.7427991886409736\n",
      "auc roc score corresponding to  61  is  0.7620689655172413\n",
      "auc roc score corresponding to  62  is  0.7925287356321838\n",
      "auc roc score corresponding to  63  is  0.7160919540229885\n",
      "auc roc score corresponding to  64  is  0.7982758620689655\n",
      "auc roc score corresponding to  65  is  0.7275862068965516\n",
      "auc roc score corresponding to  66  is  0.790500338066261\n",
      "auc roc score corresponding to  67  is  0.7431034482758622\n",
      "auc roc score corresponding to  68  is  0.7609195402298851\n",
      "auc roc score corresponding to  69  is  0.7494252873563219\n",
      "auc roc score corresponding to  70  is  0.7172413793103448\n",
      "auc roc score corresponding to  71  is  0.780155510480054\n",
      "auc roc score corresponding to  72  is  0.7790060851926978\n",
      "auc roc score corresponding to  73  is  0.7390804597701148\n",
      "auc roc score corresponding to  74  is  0.6606152805949966\n",
      "auc roc score corresponding to  75  is  0.7867816091954024\n",
      "auc roc score corresponding to  76  is  0.783029073698445\n",
      "auc roc score corresponding to  77  is  0.8085868830290738\n",
      "auc roc score corresponding to  78  is  0.7382014874915482\n",
      "auc roc score corresponding to  79  is  0.6778566599053414\n",
      "auc roc score corresponding to  80  is  0.7028735632183908\n",
      "auc roc score corresponding to  81  is  0.7459770114942529\n",
      "auc roc score corresponding to  82  is  0.6703853955375253\n",
      "auc roc score corresponding to  83  is  0.7120351588911427\n",
      "auc roc score corresponding to  84  is  0.7571670047329278\n",
      "auc roc score corresponding to  85  is  0.7094658553076402\n",
      "auc roc score corresponding to  86  is  0.7226842461122379\n",
      "auc roc score corresponding to  87  is  0.7008451656524679\n",
      "auc roc score corresponding to  88  is  0.6873563218390805\n",
      "auc roc score corresponding to  89  is  0.6832995267072346\n",
      "auc roc score corresponding to  90  is  0.7678160919540229\n",
      "auc roc score corresponding to  91  is  0.7761325219743069\n",
      "auc roc score corresponding to  92  is  0.7209601081812035\n",
      "auc roc score corresponding to  93  is  0.7712643678160919\n",
      "auc roc score corresponding to  94  is  0.767816091954023\n",
      "auc roc score corresponding to  95  is  0.7431034482758619\n",
      "auc roc score corresponding to  96  is  0.7649425287356322\n",
      "auc roc score corresponding to  97  is  0.6864773495605139\n",
      "auc roc score corresponding to  98  is  0.7402298850574712\n",
      "auc roc score corresponding to  99  is  0.7764367816091954\n",
      "max auc roc score corresponding to  56  is  0.8106152805949968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc=KNeighborsClassifier(n_neighbors=25)\n",
    "max_aucroc_score(kc,x,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN CLASSIFIER\n",
      "\n",
      "\n",
      "Mean roc auc score for classifier:  0.936264062648981\n",
      "standard deviation in roc auc score for classifier:  0.029589198947794684\n",
      "[0.92966274 0.89980114 0.93850559 0.93064557 0.9654808  0.95717189\n",
      " 0.95080532 0.97637047 0.87150704 0.94269006]\n"
     ]
    }
   ],
   "source": [
    "#Lets chcek the cross val score\n",
    "#lets print the scores for KNN classifier\n",
    "print(\"KNN CLASSIFIER\\n\\n\")\n",
    "model_evaluation(kc,x,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc roc score corresponding to  42  is  0.6915483434753211\n",
      "auc roc score corresponding to  43  is  0.7760649087221095\n",
      "auc roc score corresponding to  44  is  0.7846855983772819\n",
      "auc roc score corresponding to  45  is  0.7317782285327924\n",
      "auc roc score corresponding to  46  is  0.7884381338742393\n",
      "auc roc score corresponding to  47  is  0.7412778904665314\n",
      "auc roc score corresponding to  48  is  0.7576402974983095\n",
      "auc roc score corresponding to  49  is  0.9039891818796484\n",
      "auc roc score corresponding to  50  is  0.8821501014198784\n",
      "auc roc score corresponding to  51  is  0.8361392832995268\n",
      "auc roc score corresponding to  52  is  0.7564908722109532\n",
      "auc roc score corresponding to  53  is  0.7527721433400947\n",
      "auc roc score corresponding to  54  is  0.6910074374577416\n",
      "auc roc score corresponding to  55  is  0.7596686950642325\n",
      "auc roc score corresponding to  56  is  0.8467545638945233\n",
      "auc roc score corresponding to  57  is  0.7530764029749831\n",
      "auc roc score corresponding to  58  is  0.7597025016903313\n",
      "auc roc score corresponding to  59  is  0.7145030425963489\n",
      "auc roc score corresponding to  60  is  0.8016227180527382\n",
      "auc roc score corresponding to  61  is  0.7493576741041243\n",
      "auc roc score corresponding to  62  is  0.8122718052738337\n",
      "auc roc score corresponding to  63  is  0.7490534144692359\n",
      "auc roc score corresponding to  64  is  0.7510480054090601\n",
      "auc roc score corresponding to  65  is  0.7070993914807303\n",
      "auc roc score corresponding to  66  is  0.7096348884381337\n",
      "auc roc score corresponding to  67  is  0.7510480054090601\n",
      "auc roc score corresponding to  68  is  0.7872549019607843\n",
      "auc roc score corresponding to  69  is  0.7490534144692359\n",
      "auc roc score corresponding to  70  is  0.7913116970926302\n",
      "auc roc score corresponding to  71  is  0.8036511156186611\n",
      "auc roc score corresponding to  72  is  0.7864097363083165\n",
      "auc roc score corresponding to  73  is  0.7990872210953346\n",
      "auc roc score corresponding to  74  is  0.7766058147396889\n",
      "auc roc score corresponding to  75  is  0.8985463150777552\n",
      "auc roc score corresponding to  76  is  0.7809668695064232\n",
      "auc roc score corresponding to  77  is  0.8553752535496958\n",
      "auc roc score corresponding to  78  is  0.7786342123056119\n",
      "auc roc score corresponding to  79  is  0.7835361730899254\n",
      "auc roc score corresponding to  80  is  0.7691683569979716\n",
      "auc roc score corresponding to  81  is  0.785868830290737\n",
      "auc roc score corresponding to  82  is  0.7893171061528058\n",
      "auc roc score corresponding to  83  is  0.7634212305611899\n",
      "auc roc score corresponding to  84  is  0.7084178498985801\n",
      "auc roc score corresponding to  85  is  0.8257944557133198\n",
      "auc roc score corresponding to  86  is  0.7654496281271129\n",
      "auc roc score corresponding to  87  is  0.7953346855983772\n",
      "auc roc score corresponding to  88  is  0.7564908722109532\n",
      "auc roc score corresponding to  89  is  0.6932386747802569\n",
      "auc roc score corresponding to  90  is  0.816869506423259\n",
      "auc roc score corresponding to  91  is  0.9031440162271805\n",
      "auc roc score corresponding to  92  is  0.7386747802569303\n",
      "auc roc score corresponding to  93  is  0.8973968897903989\n",
      "auc roc score corresponding to  94  is  0.7585192697768762\n",
      "auc roc score corresponding to  95  is  0.8275185936443542\n",
      "auc roc score corresponding to  96  is  0.9059837728194727\n",
      "auc roc score corresponding to  97  is  0.7671399594320486\n",
      "auc roc score corresponding to  98  is  0.7309330628803244\n",
      "auc roc score corresponding to  99  is  0.8763691683569981\n",
      "max auc roc score corresponding to  96  is  0.9059837728194727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets use decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dc=DecisionTreeClassifier()\n",
    "max_aucroc_score(dc,df_x,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree CLASSIFIER\n",
      "\n",
      "\n",
      "Mean roc auc score for classifier:  0.7293307925747667\n",
      "standard deviation in roc auc score for classifier:  0.061934298061911974\n",
      "[0.77162658 0.64670611 0.75446844 0.76644732 0.7592943  0.74498178\n",
      " 0.74842437 0.76842028 0.76902657 0.57284076]\n"
     ]
    }
   ],
   "source": [
    "#lets use cross_val_score\n",
    "#Lets print the scores of decision tree\n",
    "print(\"Decision tree CLASSIFIER\\n\\n\")\n",
    "model_evaluation(dc,df_x,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets use random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters={\"n_estimators\":[10,100,500]}\n",
    "rf_clf=RandomForestClassifier()\n",
    "clf = GridSearchCV(rf_clf, parameters, cv=10,scoring=\"roc_auc\")\n",
    "clf.fit(df_x,y1)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc roc score corresponding to  42  is  0.8755578093306288\n",
      "auc roc score corresponding to  43  is  0.7780933062880324\n",
      "auc roc score corresponding to  44  is  0.785868830290737\n",
      "auc roc score corresponding to  45  is  0.8459432048681541\n",
      "auc roc score corresponding to  46  is  0.7723461798512509\n",
      "auc roc score corresponding to  47  is  0.7818458417849898\n",
      "auc roc score corresponding to  48  is  0.7645706558485462\n",
      "auc roc score corresponding to  49  is  0.8764029749830966\n",
      "auc roc score corresponding to  50  is  0.8519945909398242\n",
      "auc roc score corresponding to  51  is  0.8574374577417173\n",
      "auc roc score corresponding to  52  is  0.8600067613252197\n",
      "auc roc score corresponding to  53  is  0.779817444219067\n",
      "auc roc score corresponding to  54  is  0.7542596348884382\n",
      "auc roc score corresponding to  55  is  0.8192021636240702\n",
      "auc roc score corresponding to  56  is  0.8315415821501014\n",
      "auc roc score corresponding to  57  is  0.8048343475321164\n",
      "auc roc score corresponding to  58  is  0.7732251521298174\n",
      "auc roc score corresponding to  59  is  0.7062204192021636\n",
      "auc roc score corresponding to  60  is  0.8028059499661934\n",
      "auc roc score corresponding to  61  is  0.7427315753887762\n",
      "auc roc score corresponding to  62  is  0.8381676808654497\n",
      "auc roc score corresponding to  63  is  0.7876267748478701\n",
      "auc roc score corresponding to  64  is  0.7962136578769439\n",
      "auc roc score corresponding to  65  is  0.808891142663962\n",
      "auc roc score corresponding to  66  is  0.785868830290737\n",
      "auc roc score corresponding to  67  is  0.785868830290737\n",
      "auc roc score corresponding to  68  is  0.7254901960784313\n",
      "auc roc score corresponding to  69  is  0.756288032454361\n",
      "auc roc score corresponding to  70  is  0.8927991886409736\n",
      "auc roc score corresponding to  71  is  0.8508113590263692\n",
      "auc roc score corresponding to  72  is  0.863184584178499\n",
      "auc roc score corresponding to  73  is  0.8594658553076403\n",
      "auc roc score corresponding to  74  is  0.8192021636240702\n",
      "auc roc score corresponding to  75  is  0.8709601081812034\n",
      "auc roc score corresponding to  76  is  0.8364435429344151\n",
      "auc roc score corresponding to  77  is  0.8623056118999325\n",
      "auc roc score corresponding to  78  is  0.8290060851926978\n",
      "auc roc score corresponding to  79  is  0.8401960784313726\n",
      "auc roc score corresponding to  80  is  0.8669371196754564\n",
      "auc roc score corresponding to  81  is  0.8488167680865449\n",
      "auc roc score corresponding to  82  is  0.7904665314401623\n",
      "auc roc score corresponding to  83  is  0.8057133198106828\n",
      "auc roc score corresponding to  84  is  0.7395199459093981\n",
      "auc roc score corresponding to  85  is  0.8249492900608519\n",
      "auc roc score corresponding to  86  is  0.8212305611899932\n",
      "auc roc score corresponding to  87  is  0.7723461798512509\n",
      "auc roc score corresponding to  88  is  0.8287018255578094\n",
      "auc roc score corresponding to  89  is  0.793340094658553\n",
      "auc roc score corresponding to  90  is  0.7749154834347532\n",
      "auc roc score corresponding to  91  is  0.8956727518593643\n",
      "auc roc score corresponding to  92  is  0.798208248816768\n",
      "auc roc score corresponding to  93  is  0.811764705882353\n",
      "auc roc score corresponding to  94  is  0.7904665314401621\n",
      "auc roc score corresponding to  95  is  0.7608519269776876\n",
      "auc roc score corresponding to  96  is  0.7973630831643002\n",
      "auc roc score corresponding to  97  is  0.7970588235294117\n",
      "auc roc score corresponding to  98  is  0.7283637592968222\n",
      "auc roc score corresponding to  99  is  0.8442190669371197\n",
      "max auc roc score corresponding to  91  is  0.8956727518593643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf=RandomForestClassifier(n_estimators=500)\n",
    "max_aucroc_score(rf_clf,df_x,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest CLASSIFIER\n",
      "\n",
      "\n",
      "Mean roc auc score for classifier:  0.8852324494038177\n",
      "standard deviation in roc auc score for classifier:  0.028035801371429383\n",
      "[0.88423583 0.83587462 0.84193585 0.85818671 0.86629675 0.88760462\n",
      " 0.85921452 0.90368827 0.93198093 0.93714575]\n"
     ]
    }
   ],
   "source": [
    "#lets chcek the cross_val score\n",
    "#Lets print the scores of Random forest\n",
    "print(\"Random forest CLASSIFIER\\n\\n\")\n",
    "model_evaluation(rf_clf,df_x,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use random forest as a final model\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, y,random_state = 91,test_size=0.10,stratify=y)\n",
    "x_train, y_train = SMOTE().fit_sample(x_train, y_train)\n",
    "rf_clf=RandomForestClassifier(n_estimators=500)\n",
    "rf_clf.fit(x_train,y_train)\n",
    "y_pred=rf_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[ 2  2  1]\n",
      " [ 2 27  0]\n",
      " [ 5  0 24]]\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.40      0.29         5\n",
      "           1       0.93      0.93      0.93        29\n",
      "           2       0.96      0.83      0.89        29\n",
      "\n",
      "   micro avg       0.84      0.84      0.84        63\n",
      "   macro avg       0.70      0.72      0.70        63\n",
      "weighted avg       0.89      0.84      0.86        63\n",
      "\n",
      "AUC ROC Score:  0.8249492900608519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"Confusion matrix \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"classification report \\n\",classification_report(y_test,y_pred))\n",
    "y_test = label_binarize(y_test, classes=[0, 1, 2])\n",
    "y_pred = label_binarize(y_pred, classes=[0, 1, 2])\n",
    "print(\"AUC ROC Score: \",roc_auc_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.621600</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.807300</td>\n",
       "      <td>-0.446990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.545900</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.458600</td>\n",
       "      <td>-1.462100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.866000</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.924200</td>\n",
       "      <td>0.106450</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.456600</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.011200</td>\n",
       "      <td>-3.594400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.329240</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.571800</td>\n",
       "      <td>-0.988800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.368400</td>\n",
       "      <td>9.67180</td>\n",
       "      <td>-3.960600</td>\n",
       "      <td>-3.162500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.591200</td>\n",
       "      <td>3.01290</td>\n",
       "      <td>0.728880</td>\n",
       "      <td>0.564210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.092200</td>\n",
       "      <td>-6.81000</td>\n",
       "      <td>8.463600</td>\n",
       "      <td>-0.602160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.203200</td>\n",
       "      <td>5.75880</td>\n",
       "      <td>-0.753450</td>\n",
       "      <td>-0.612510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.535600</td>\n",
       "      <td>9.17720</td>\n",
       "      <td>-2.271800</td>\n",
       "      <td>-0.735350</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.224700</td>\n",
       "      <td>8.77790</td>\n",
       "      <td>-2.213500</td>\n",
       "      <td>-0.806470</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.989900</td>\n",
       "      <td>-2.70660</td>\n",
       "      <td>2.394600</td>\n",
       "      <td>0.862910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.899300</td>\n",
       "      <td>7.66250</td>\n",
       "      <td>0.153940</td>\n",
       "      <td>-3.110800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.576800</td>\n",
       "      <td>10.84300</td>\n",
       "      <td>2.546200</td>\n",
       "      <td>-2.936200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.404000</td>\n",
       "      <td>8.72610</td>\n",
       "      <td>-2.991500</td>\n",
       "      <td>-0.572420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.676500</td>\n",
       "      <td>-3.38950</td>\n",
       "      <td>3.489600</td>\n",
       "      <td>1.477100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.671900</td>\n",
       "      <td>3.06460</td>\n",
       "      <td>0.371580</td>\n",
       "      <td>0.586190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.803550</td>\n",
       "      <td>2.84730</td>\n",
       "      <td>4.343900</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.447900</td>\n",
       "      <td>-4.87940</td>\n",
       "      <td>8.342800</td>\n",
       "      <td>-2.108600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.242300</td>\n",
       "      <td>11.02720</td>\n",
       "      <td>-4.353000</td>\n",
       "      <td>-4.101300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.786700</td>\n",
       "      <td>7.89020</td>\n",
       "      <td>-2.619600</td>\n",
       "      <td>-0.487080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.329200</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.571800</td>\n",
       "      <td>-0.988800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.936200</td>\n",
       "      <td>10.16220</td>\n",
       "      <td>-3.823500</td>\n",
       "      <td>-4.017200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.935840</td>\n",
       "      <td>8.88550</td>\n",
       "      <td>-1.683100</td>\n",
       "      <td>-1.659900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.433800</td>\n",
       "      <td>9.88700</td>\n",
       "      <td>-4.679500</td>\n",
       "      <td>-3.748300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.705700</td>\n",
       "      <td>-5.49810</td>\n",
       "      <td>8.336800</td>\n",
       "      <td>-2.871500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.143200</td>\n",
       "      <td>-3.74130</td>\n",
       "      <td>5.577700</td>\n",
       "      <td>-0.635780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.382140</td>\n",
       "      <td>8.39090</td>\n",
       "      <td>2.162400</td>\n",
       "      <td>-3.740500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.563300</td>\n",
       "      <td>9.81870</td>\n",
       "      <td>-4.411300</td>\n",
       "      <td>-3.225800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.890600</td>\n",
       "      <td>-3.35840</td>\n",
       "      <td>3.420200</td>\n",
       "      <td>1.090500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>-1.747900</td>\n",
       "      <td>-5.82300</td>\n",
       "      <td>5.869900</td>\n",
       "      <td>1.212000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>-0.959230</td>\n",
       "      <td>-6.71280</td>\n",
       "      <td>4.985700</td>\n",
       "      <td>0.328860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>1.345100</td>\n",
       "      <td>0.23589</td>\n",
       "      <td>-1.878500</td>\n",
       "      <td>1.325800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>2.227900</td>\n",
       "      <td>4.09510</td>\n",
       "      <td>-4.803700</td>\n",
       "      <td>-2.111200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>1.257200</td>\n",
       "      <td>4.87310</td>\n",
       "      <td>-5.286100</td>\n",
       "      <td>-5.874100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>-5.385700</td>\n",
       "      <td>9.12140</td>\n",
       "      <td>-0.419290</td>\n",
       "      <td>-5.918100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>-2.978600</td>\n",
       "      <td>2.34450</td>\n",
       "      <td>0.526670</td>\n",
       "      <td>-0.401730</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>-1.585100</td>\n",
       "      <td>-2.15620</td>\n",
       "      <td>1.708200</td>\n",
       "      <td>0.901700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>-0.218880</td>\n",
       "      <td>-2.20380</td>\n",
       "      <td>-0.095400</td>\n",
       "      <td>0.564210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>1.318300</td>\n",
       "      <td>1.90170</td>\n",
       "      <td>-3.311100</td>\n",
       "      <td>0.065071</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>1.489600</td>\n",
       "      <td>3.42880</td>\n",
       "      <td>-4.030900</td>\n",
       "      <td>-1.425900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>0.115920</td>\n",
       "      <td>3.22190</td>\n",
       "      <td>-3.430200</td>\n",
       "      <td>-2.845700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>-3.392400</td>\n",
       "      <td>3.35640</td>\n",
       "      <td>-0.720040</td>\n",
       "      <td>-3.523300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>-6.163200</td>\n",
       "      <td>8.70960</td>\n",
       "      <td>-0.216210</td>\n",
       "      <td>-3.634500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>-4.078600</td>\n",
       "      <td>2.92390</td>\n",
       "      <td>0.870260</td>\n",
       "      <td>-0.653890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>-2.589900</td>\n",
       "      <td>-0.39110</td>\n",
       "      <td>0.934520</td>\n",
       "      <td>0.429720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>-1.011600</td>\n",
       "      <td>-0.19038</td>\n",
       "      <td>-0.905970</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>0.066129</td>\n",
       "      <td>2.49140</td>\n",
       "      <td>-2.940100</td>\n",
       "      <td>-0.621560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>-0.247450</td>\n",
       "      <td>1.93680</td>\n",
       "      <td>-2.469700</td>\n",
       "      <td>-0.805180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>-1.573200</td>\n",
       "      <td>1.06360</td>\n",
       "      <td>-0.712320</td>\n",
       "      <td>-0.838800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>-2.166800</td>\n",
       "      <td>1.59330</td>\n",
       "      <td>0.045122</td>\n",
       "      <td>-1.678000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>-1.166700</td>\n",
       "      <td>-1.42370</td>\n",
       "      <td>2.924100</td>\n",
       "      <td>0.661190</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>-2.839100</td>\n",
       "      <td>-6.63000</td>\n",
       "      <td>10.484900</td>\n",
       "      <td>-0.421130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>-4.504600</td>\n",
       "      <td>-5.81260</td>\n",
       "      <td>10.886700</td>\n",
       "      <td>-0.528460</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>-2.410000</td>\n",
       "      <td>3.74330</td>\n",
       "      <td>-0.402150</td>\n",
       "      <td>-1.295300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.406140</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.450100</td>\n",
       "      <td>-0.559490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.388700</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.477400</td>\n",
       "      <td>0.341790</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.750300</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.593200</td>\n",
       "      <td>-2.777100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.563700</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.393000</td>\n",
       "      <td>-1.282300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.541900</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.684200</td>\n",
       "      <td>1.195200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1          2         3  4\n",
       "0     3.621600   8.66610  -2.807300 -0.446990  0\n",
       "1     4.545900   8.16740  -2.458600 -1.462100  0\n",
       "2     3.866000  -2.63830   1.924200  0.106450  0\n",
       "3     3.456600   9.52280  -4.011200 -3.594400  0\n",
       "4     0.329240  -4.45520   4.571800 -0.988800  0\n",
       "5     4.368400   9.67180  -3.960600 -3.162500  0\n",
       "6     3.591200   3.01290   0.728880  0.564210  0\n",
       "7     2.092200  -6.81000   8.463600 -0.602160  0\n",
       "8     3.203200   5.75880  -0.753450 -0.612510  0\n",
       "9     1.535600   9.17720  -2.271800 -0.735350  0\n",
       "10    1.224700   8.77790  -2.213500 -0.806470  0\n",
       "11    3.989900  -2.70660   2.394600  0.862910  0\n",
       "12    1.899300   7.66250   0.153940 -3.110800  0\n",
       "13   -1.576800  10.84300   2.546200 -2.936200  0\n",
       "14    3.404000   8.72610  -2.991500 -0.572420  0\n",
       "15    4.676500  -3.38950   3.489600  1.477100  0\n",
       "16    2.671900   3.06460   0.371580  0.586190  0\n",
       "17    0.803550   2.84730   4.343900  0.601700  0\n",
       "18    1.447900  -4.87940   8.342800 -2.108600  0\n",
       "19    5.242300  11.02720  -4.353000 -4.101300  0\n",
       "20    5.786700   7.89020  -2.619600 -0.487080  0\n",
       "21    0.329200  -4.45520   4.571800 -0.988800  0\n",
       "22    3.936200  10.16220  -3.823500 -4.017200  0\n",
       "23    0.935840   8.88550  -1.683100 -1.659900  0\n",
       "24    4.433800   9.88700  -4.679500 -3.748300  0\n",
       "25    0.705700  -5.49810   8.336800 -2.871500  0\n",
       "26    1.143200  -3.74130   5.577700 -0.635780  0\n",
       "27   -0.382140   8.39090   2.162400 -3.740500  0\n",
       "28    6.563300   9.81870  -4.411300 -3.225800  0\n",
       "29    4.890600  -3.35840   3.420200  1.090500  0\n",
       "...        ...       ...        ...       ... ..\n",
       "1342 -1.747900  -5.82300   5.869900  1.212000  1\n",
       "1343 -0.959230  -6.71280   4.985700  0.328860  1\n",
       "1344  1.345100   0.23589  -1.878500  1.325800  1\n",
       "1345  2.227900   4.09510  -4.803700 -2.111200  1\n",
       "1346  1.257200   4.87310  -5.286100 -5.874100  1\n",
       "1347 -5.385700   9.12140  -0.419290 -5.918100  1\n",
       "1348 -2.978600   2.34450   0.526670 -0.401730  1\n",
       "1349 -1.585100  -2.15620   1.708200  0.901700  1\n",
       "1350 -0.218880  -2.20380  -0.095400  0.564210  1\n",
       "1351  1.318300   1.90170  -3.311100  0.065071  1\n",
       "1352  1.489600   3.42880  -4.030900 -1.425900  1\n",
       "1353  0.115920   3.22190  -3.430200 -2.845700  1\n",
       "1354 -3.392400   3.35640  -0.720040 -3.523300  1\n",
       "1355 -6.163200   8.70960  -0.216210 -3.634500  1\n",
       "1356 -4.078600   2.92390   0.870260 -0.653890  1\n",
       "1357 -2.589900  -0.39110   0.934520  0.429720  1\n",
       "1358 -1.011600  -0.19038  -0.905970  0.003003  1\n",
       "1359  0.066129   2.49140  -2.940100 -0.621560  1\n",
       "1360 -0.247450   1.93680  -2.469700 -0.805180  1\n",
       "1361 -1.573200   1.06360  -0.712320 -0.838800  1\n",
       "1362 -2.166800   1.59330   0.045122 -1.678000  1\n",
       "1363 -1.166700  -1.42370   2.924100  0.661190  1\n",
       "1364 -2.839100  -6.63000  10.484900 -0.421130  1\n",
       "1365 -4.504600  -5.81260  10.886700 -0.528460  1\n",
       "1366 -2.410000   3.74330  -0.402150 -1.295300  1\n",
       "1367  0.406140   1.34920  -1.450100 -0.559490  1\n",
       "1368 -1.388700  -4.87730   6.477400  0.341790  1\n",
       "1369 -3.750300 -13.45860  17.593200 -2.777100  1\n",
       "1370 -3.563700  -8.38270  12.393000 -1.282300  1\n",
       "1371 -2.541900  -0.65804   2.684200  1.195200  1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_notes=pd.read_csv(\"data_banknote_authentication.txt\",sep=\",\",header=None)\n",
    "df_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets give name to columns according to data set description\n",
    "df_notes.columns=[\"variance\",\"skewness\",\"curtosis\",\"entropy\",\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance    0\n",
       "skewness    0\n",
       "curtosis    0\n",
       "entropy     0\n",
       "class       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check whether there are null values or not\n",
    "df_notes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>variance</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.264026</td>\n",
       "      <td>-0.380850</td>\n",
       "      <td>0.276817</td>\n",
       "      <td>-0.724843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skewness</th>\n",
       "      <td>0.264026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.786895</td>\n",
       "      <td>-0.526321</td>\n",
       "      <td>-0.444688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curtosis</th>\n",
       "      <td>-0.380850</td>\n",
       "      <td>-0.786895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.155883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entropy</th>\n",
       "      <td>0.276817</td>\n",
       "      <td>-0.526321</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.023424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>-0.724843</td>\n",
       "      <td>-0.444688</td>\n",
       "      <td>0.155883</td>\n",
       "      <td>-0.023424</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variance  skewness  curtosis   entropy     class\n",
       "variance  1.000000  0.264026 -0.380850  0.276817 -0.724843\n",
       "skewness  0.264026  1.000000 -0.786895 -0.526321 -0.444688\n",
       "curtosis -0.380850 -0.786895  1.000000  0.318841  0.155883\n",
       "entropy   0.276817 -0.526321  0.318841  1.000000 -0.023424\n",
       "class    -0.724843 -0.444688  0.155883 -0.023424  1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets chcek the correlation matrix\n",
    "df_notes.corr()\n",
    "#from below correlation matrix we see thee is strong negative correlation bewteen class and variance of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.433735</td>\n",
       "      <td>1.922353</td>\n",
       "      <td>1.397627</td>\n",
       "      <td>-1.191657</td>\n",
       "      <td>0.444606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.842763</td>\n",
       "      <td>5.869047</td>\n",
       "      <td>4.310030</td>\n",
       "      <td>2.101013</td>\n",
       "      <td>0.497103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.042100</td>\n",
       "      <td>-13.773100</td>\n",
       "      <td>-5.286100</td>\n",
       "      <td>-8.548200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.773000</td>\n",
       "      <td>-1.708200</td>\n",
       "      <td>-1.574975</td>\n",
       "      <td>-2.413450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.496180</td>\n",
       "      <td>2.319650</td>\n",
       "      <td>0.616630</td>\n",
       "      <td>-0.586650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.821475</td>\n",
       "      <td>6.814625</td>\n",
       "      <td>3.179250</td>\n",
       "      <td>0.394810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.824800</td>\n",
       "      <td>12.951600</td>\n",
       "      <td>17.927400</td>\n",
       "      <td>2.449500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variance     skewness     curtosis      entropy        class\n",
       "count  1372.000000  1372.000000  1372.000000  1372.000000  1372.000000\n",
       "mean      0.433735     1.922353     1.397627    -1.191657     0.444606\n",
       "std       2.842763     5.869047     4.310030     2.101013     0.497103\n",
       "min      -7.042100   -13.773100    -5.286100    -8.548200     0.000000\n",
       "25%      -1.773000    -1.708200    -1.574975    -2.413450     0.000000\n",
       "50%       0.496180     2.319650     0.616630    -0.586650     0.000000\n",
       "75%       2.821475     6.814625     3.179250     0.394810     1.000000\n",
       "max       6.824800    12.951600    17.927400     2.449500     1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets describe the matrix\n",
    "df_notes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets chcek the proportion of each type of notes in dataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x=\"class\",data=df_notes)\n",
    "plt.show()\n",
    "#tr is almost balnaced dataset having significnat cout of each type of notes for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before removing the outliers : (1372, 5)\n",
      "shape after removing the outliers : (1336, 5)\n"
     ]
    }
   ],
   "source": [
    "#lets check the presence of outlier\n",
    "from scipy.stats import zscore\n",
    "print(\"shape before removing the outliers :\",df_notes.shape)\n",
    "z_scr=zscore(df_notes)\n",
    "df_notes_new=df_notes.loc[(abs(z_scr)<3).all(axis=1)]\n",
    "print(\"shape after removing the outliers :\",df_notes_new.shape)\n",
    "#we see here 36 rows have been dropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets again chcek the count of each type of class of notes\n",
    "sns.countplot(x=\"class\",data=df_notes_new)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets divide the new dataet into input and output datsets\n",
    "df_x=df_notes_new.drop(columns=[\"class\"])\n",
    "y=df_notes_new[[\"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance   -0.165024\n",
       "skewness   -0.298058\n",
       "curtosis    0.788075\n",
       "entropy    -0.935068\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check the skewness in the dataset\n",
    "df_x.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets try to treat the skewness\n",
    "import numpy as np\n",
    "for index in df_x.skew().index:\n",
    "    if df_x.skew().loc[index]>0.5:\n",
    "        df_x[index]=np.cbrt(df_x[index])\n",
    "    if df_x.skew().loc[index]<-0.5:\n",
    "        df_x[index]=np.cbrt(df_x[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variance   -0.165024\n",
       "skewness   -0.298058\n",
       "curtosis   -0.144276\n",
       "entropy     0.367667\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets again chcek the skewness\n",
    "df_x.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets bring all features to a common scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "sc.fit(df_x)\n",
    "x=sc.transform(df_x)\n",
    "x=pd.DataFrame(x,columns=df_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum f1 score in between random states 42 to 100\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "def maxf1_score(clf,df_x,y):\n",
    "    maxf=0\n",
    "    rs=0\n",
    "    for r_state in range(42,100):\n",
    "        x_train,x_test,y_train,y_test=train_test_split(df_x, y,random_state = r_state,test_size=0.20,stratify=y)\n",
    "        clf.fit(x_train,y_train)\n",
    "        y_pred=clf.predict(x_test)\n",
    "        tmp=f1_score(y_test,y_pred)\n",
    "        print(\"random state :\",r_state,\" and f1 score: \",tmp)\n",
    "        if tmp>maxf:\n",
    "            maxf=tmp\n",
    "            rs=r_state\n",
    "    print(\"maximum f1_score is at random state :\",rs,\" and it is :\",maxf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random state : 42  and f1 score:  0.9957446808510638\n",
      "random state : 43  and f1 score:  1.0\n",
      "random state : 44  and f1 score:  0.9957446808510638\n",
      "random state : 45  and f1 score:  0.9914529914529915\n",
      "random state : 46  and f1 score:  0.9872340425531915\n",
      "random state : 47  and f1 score:  0.9873417721518987\n",
      "random state : 48  and f1 score:  0.9829059829059829\n",
      "random state : 49  and f1 score:  0.983050847457627\n",
      "random state : 50  and f1 score:  0.9915254237288135\n",
      "random state : 51  and f1 score:  0.9873417721518987\n",
      "random state : 52  and f1 score:  0.9831932773109243\n",
      "random state : 53  and f1 score:  0.9873417721518987\n",
      "random state : 54  and f1 score:  0.9873417721518987\n",
      "random state : 55  and f1 score:  0.9873417721518987\n",
      "random state : 56  and f1 score:  0.9957446808510638\n",
      "random state : 57  and f1 score:  0.9914529914529915\n",
      "random state : 58  and f1 score:  0.9872340425531915\n",
      "random state : 59  and f1 score:  0.9957446808510638\n",
      "random state : 60  and f1 score:  0.9914529914529915\n",
      "random state : 61  and f1 score:  0.983050847457627\n",
      "random state : 62  and f1 score:  1.0\n",
      "random state : 63  and f1 score:  0.9789029535864978\n",
      "random state : 64  and f1 score:  0.9872340425531915\n",
      "random state : 65  and f1 score:  0.9872340425531915\n",
      "random state : 66  and f1 score:  0.9957446808510638\n",
      "random state : 67  and f1 score:  0.9873417721518987\n",
      "random state : 68  and f1 score:  0.9914529914529915\n",
      "random state : 69  and f1 score:  0.9957081545064378\n",
      "random state : 70  and f1 score:  0.9957446808510638\n",
      "random state : 71  and f1 score:  0.983050847457627\n",
      "random state : 72  and f1 score:  0.9872340425531915\n",
      "random state : 73  and f1 score:  0.9872340425531915\n",
      "random state : 74  and f1 score:  0.9871244635193134\n",
      "random state : 75  and f1 score:  0.9873417721518987\n",
      "random state : 76  and f1 score:  0.9915254237288135\n",
      "random state : 77  and f1 score:  0.9957081545064378\n",
      "random state : 78  and f1 score:  0.9743589743589743\n",
      "random state : 79  and f1 score:  0.9743589743589743\n",
      "random state : 80  and f1 score:  0.9957081545064378\n",
      "random state : 81  and f1 score:  0.9957446808510638\n",
      "random state : 82  and f1 score:  0.9913793103448275\n",
      "random state : 83  and f1 score:  0.983050847457627\n",
      "random state : 84  and f1 score:  0.9871244635193134\n",
      "random state : 85  and f1 score:  0.9957081545064378\n",
      "random state : 86  and f1 score:  0.9831932773109243\n",
      "random state : 87  and f1 score:  0.9872340425531915\n",
      "random state : 88  and f1 score:  0.9913793103448275\n",
      "random state : 89  and f1 score:  0.9914529914529915\n",
      "random state : 90  and f1 score:  0.9957446808510638\n",
      "random state : 91  and f1 score:  1.0\n",
      "random state : 92  and f1 score:  0.9957081545064378\n",
      "random state : 93  and f1 score:  0.9914529914529915\n",
      "random state : 94  and f1 score:  0.9915254237288135\n",
      "random state : 95  and f1 score:  0.9789029535864978\n",
      "random state : 96  and f1 score:  0.9871244635193134\n",
      "random state : 97  and f1 score:  0.983050847457627\n",
      "random state : 98  and f1 score:  0.9915254237288135\n",
      "random state : 99  and f1 score:  0.9915254237288135\n",
      "maximum f1_score is at random state : 43  and it is : 1.0\n"
     ]
    }
   ],
   "source": [
    "#lets use logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "lg_clf=LogisticRegression()\n",
    "maxf1_score(lg_clf,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f1 score for logistic classifier:  0.9906164757228588\n",
      "standard deviation in f1 score for logistic classifier:  0.0049837593815716535\n",
      "[0.99574468 0.98723404 0.98290598 0.99574468 0.99145299]\n"
     ]
    }
   ],
   "source": [
    "#Now lets use cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(\"Mean f1 score for logistic classifier: \",cross_val_score(lg_clf,x,y,cv=5,scoring=\"f1\").mean())\n",
    "print(\"standard deviation in f1 score for logistic classifier: \",cross_val_score(lg_clf,x,y,cv=5,scoring=\"f1\").std())\n",
    "print(cross_val_score(lg_clf,x,y,cv=5,scoring=\"f1\"))\n",
    "#Based on the below o/p we can choose this model itself as it is performing very well on different combs of training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets choose logistic regression final model and random state 43\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,random_state = 70,test_size=0.20,stratify=y)\n",
    "lg_clf=LogisticRegression()\n",
    "lg_clf.fit(x_train,y_train)\n",
    "y_pred=lg_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[150   1]\n",
      " [  0 117]]\n",
      "f1 score is :  0.9957446808510638\n",
      "classification report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       151\n",
      "           1       0.99      1.00      1.00       117\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       268\n",
      "   macro avg       1.00      1.00      1.00       268\n",
      "weighted avg       1.00      1.00      1.00       268\n",
      "\n",
      "AUC ROC Score:  0.9966887417218543\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"Confusion matrix \\n\",confusion_matrix(y_test,y_pred))\n",
    "print(\"f1 score is : \",f1_score(y_test,y_pred))\n",
    "print(\"classification report \\n\",classification_report(y_test,y_pred))\n",
    "print(\"AUC ROC Score: \",roc_auc_score(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
